{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9egyhuvU3_GI"
      },
      "source": [
        "# PyTerrier Tutorial Notebook - Neural Re-Ranking\n",
        "\n",
        "This is one of a series of Colab notebooks created for the PyTerrier Tutorial entitled '**IR From Bag-of-words to BERT and Beyond through Practical Experiments**'. It demonstrates the use of [PyTerrier](https://github.com/terrier-org/pyterrier) on the [CORD19 test collection](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n",
        "\n",
        "In particular, in this notebook you will:\n",
        "\n",
        " - Re-rank documents using neural models like KNRM, Vanilla BERT, EPIC, and monoT5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl0-Gs6e5I7n"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In the following, we will set up the libraries required to execute the notebook.\n",
        "\n",
        "### Pyterrier installation\n",
        "\n",
        "The following cell installs the latest release of the [PyTerrier](https://github.com/terrier-org/pyterrier) package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSgDzjKxqfq5"
      },
      "outputs": [],
      "source": [
        "%pip install -q --upgrade python-terrier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV0C6jJvqhMR"
      },
      "source": [
        "### Pyterrier plugins installation \n",
        "\n",
        "We install the [OpenNIR](https://opennir.net/) and [monoT5](https://github.com/terrierteam/pyterrier_t5) PyTerrier plugins. You can safely ignore the package versioning errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkIR_PXdet7R"
      },
      "outputs": [],
      "source": [
        "%pip install -q --upgrade git+https://github.com/Georgetown-IR-Lab/OpenNIR\n",
        "%pip install -q --upgrade git+https://github.com/terrierteam/pyterrier_t5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-nQrpNP5pN7"
      },
      "source": [
        "## Preliminary steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUwC6S7QkQY"
      },
      "source": [
        "**[PyTerrier](https://github.com/terrier-org/pyterrier) initialization** \n",
        "\n",
        "Lets get [PyTerrier](https://github.com/terrier-org/pyterrier) started. This will download the latest version of the [Terrier](http://terrier.org/) IR platform. We also import the [OpenNIR](https://opennir.net/) pyterrier bindings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FegcyWr5lja"
      },
      "outputs": [],
      "source": [
        "import pyterrier as pt\n",
        "from pyterrier.measures import * # allow for natural measure names\n",
        "import onir_pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPVjr448rIPc"
      },
      "source": [
        "### [TREC-COVID19](https://ir.nist.gov/covidSubmit/) Dataset download\n",
        "\n",
        "The following cell downloads the [TREC-COVID19](https://ir.nist.gov/covidSubmit/) dataset that we will use in the remainder of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJMHFRfArd7O"
      },
      "outputs": [],
      "source": [
        "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
        "topics = dataset.get_topics(variant='description')\n",
        "qrels = dataset.get_qrels()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF3HIPhtrqOH"
      },
      "source": [
        "### Terrier inverted index download\n",
        "\n",
        "To save a few minutes, we use a pre-built Terrier inverted index for the TREC-COVID19 collection ([`'terrier_stemmed'`](http://data.terrier.org/trec-covid.dataset.html#terrier_stemmed) version). Download time took a few seconds for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oCcP90yrlGi"
      },
      "outputs": [],
      "source": [
        "index = pt.get_dataset('trec-covid').get_index('terrier_stemmed_positions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwDams5M7g6c"
      },
      "source": [
        "## Re-Rankers from scratch\n",
        "\n",
        "Let's start exploring a few neural re-ranking methods! We can build them from scratch using `onir_pt.reranker`.\n",
        "\n",
        "And OpenNIR reranking model consists of:\n",
        " - `ranker` (e.g., `drmm`, `knrm`, or `pacrr`). This defines the neural ranking architecture.\n",
        " - `vocab` (e.g., `wordvec_hash`, or `bert`). This defines how text is encoded by the model. This approach makes it easy to swap out different text representations.\n",
        "\n",
        "This line will take a few minutes to run as it downloads and prepares the word vectors.\n",
        "\n",
        "We'll start with neural ranking model that doesn't use contextualized embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0O79K2K6fvn"
      },
      "outputs": [],
      "source": [
        "knrm = onir_pt.reranker('knrm', 'wordvec_hash', text_field='title_abstract')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1avVTpxDORN"
      },
      "source": [
        "Let's look at how well these models work at ranking!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FWUIN577v1O"
      },
      "outputs": [],
      "source": [
        "br = pt.BatchRetrieve(index) % 50\n",
        "# build a sub-pipeline to get the concatenated title and abstract text\n",
        "get_title_abstract = pt.text.get_text(dataset, 'title') >> pt.text.get_text(dataset, 'abstract') >> pt.apply.title_abstract(lambda r: r['title'] + ' ' + r['abstract'])\n",
        "pipeline = br >> get_title_abstract >> knrm\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> KNRM'],\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhquobQypVNJ"
      },
      "source": [
        "This doesn't work very well because the model is not trained; it's using random weights to combine the scores from the similarity matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLv7quA43yAP"
      },
      "source": [
        "## Loading a trained re-ranker\n",
        "\n",
        "You can train re-ranking models in PyTerrier using the `fit` method. This takes a bit of time, so we'll download a model that's already been trained. If you'd like to train the model yourself, you can use:\n",
        "\n",
        "```python\n",
        "# transfer training signals from a medical sample of MS MARCO\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_ds = pt.datasets.get_dataset('irds:msmarco-passage/train/medical')\n",
        "train_topics, valid_topics = train_test_split(train_ds.get_topics(), test_size=50, random_state=42) # split into training and validation sets\n",
        "\n",
        "# Index MS MARCO\n",
        "indexer = pt.index.IterDictIndexer('./terrier_msmarco-passage')\n",
        "tr_index_ref = indexer.index(train_ds.get_corpus_iter(), fields=('text',), meta=('docno',))\n",
        "\n",
        "pipeline = (pt.BatchRetrieve(tr_index_ref) % 100 # get top 100 results\n",
        "            >> pt.text.get_text(train_ds, 'text') # fetch the document text\n",
        "            >> pt.apply.generic(lambda df: df.rename(columns={'text': 'abstract'})) # rename columns\n",
        "            >> knrm) # apply neural re-ranker\n",
        "\n",
        "pipeline.fit(\n",
        "    train_topics,\n",
        "    train_ds.get_qrels(),\n",
        "    valid_topics,\n",
        "    train_ds.get_qrels())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk7FBOgvDa8V"
      },
      "outputs": [],
      "source": [
        "del knrm # free up the memory before loading a new version of the ranker\n",
        "knrm = onir_pt.reranker.from_checkpoint('https://macavaney.us/knrm.medmarco.tar.gz', text_field='title_abstract', expected_md5=\"d70b1d4f899690dae51161537e69ed5a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BQQKv8lL0Ta"
      },
      "outputs": [],
      "source": [
        "pipeline = br >> get_title_abstract >> knrm\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> KNRM'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI1_8O8rtXKK"
      },
      "source": [
        "That's a little better than before, but it still underperforms our first-stage ranking model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79l1jn0pRQEY"
      },
      "source": [
        "## Vanilla BERT\n",
        "\n",
        "Contextualized language models, such as [BERT](https://arxiv.org/abs/1810.04805), are much more powerful neural models that have been shown to be effective for ranking.\n",
        "\n",
        "We'll try using a \"vanilla\" (or \"mono\") version of the BERT model. The BERT model is pre-trained for the task of language modeling and next sentence prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qlXPHqN3iO0"
      },
      "outputs": [],
      "source": [
        "del knrm # clear out memory from KNRM\n",
        "vbert = onir_pt.reranker('vanilla_transformer', 'bert', text_field='title_abstract', vocab_config={'train': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "progrVwaunrn"
      },
      "source": [
        "Let's see how this model does on TREC COVID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkasovrjQjy0"
      },
      "outputs": [],
      "source": [
        "pipeline = br % 50 >> get_title_abstract >> vbert\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> VBERT'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBrfNZ_1u_pD"
      },
      "source": [
        "As we see, although the model is pre-trained, it doesn't do very well at ranking on our benchmark. This is because it's not tuned for the task of relevance ranking.\n",
        "\n",
        "We can train the model for ranking (as shown above for KNRM) or we can download a trained model. Here, we use the [SLEDGE](https://arxiv.org/abs/2010.05987) model, which is a Vanilla BERT model trained on scientific text and tuned on medical queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsXQKNyYSXOj"
      },
      "outputs": [],
      "source": [
        "sledge = onir_pt.reranker.from_checkpoint('https://macavaney.us/scibert-medmarco.tar.gz', text_field='title_abstract', expected_md5=\"854966d0b61543ffffa44cea627ab63b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUH-daNJSoNy"
      },
      "outputs": [],
      "source": [
        "pipeline = br % 50 >> get_title_abstract >> sledge\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> SLEDGE'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10, 'mrt']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtAxDQHyv4ON"
      },
      "source": [
        "That's much better! We're able to significantly improve upon the first stage ranker. But we can see that this is pretty slow to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRcBOIBPRJre"
      },
      "source": [
        "## EPIC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef4O3PyAHMuS"
      },
      "source": [
        "Some models focus on query-time computational efficiency. The [EPIC](https://arxiv.org/abs/2004.14245) model builds light-weight document representations that are independent of the query. This means that they can be computed ahead of time. You can index the corpus yourself with the following code (but it takes a while):\n",
        "\n",
        "```python\n",
        "indexed_epic = onir_pt.indexed_epic.from_checkpoint('https://macavaney.us/epic.msmarco.tar.gz', index_path='./epic_cord19')\n",
        "indexed_epic.index(dataset.get_corpus_iter(), fields=('title', 'abstract'))\n",
        "```\n",
        "\n",
        "Instead, we'll download a copy of the EPIC-processed documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwkE23yfGl65"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists('epic_cord19.zip'):\n",
        "  !wget http://macavaney.us/epic_cord19.zip\n",
        "  !unzip epic_cord19.zip\n",
        "indexed_epic = onir_pt.indexed_epic.from_checkpoint('https://macavaney.us/epic.msmarco.tar.gz', index_path='./epic_cord19')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7LfbxJqw80B"
      },
      "source": [
        "We can now run this model over the results of a first-stage ranker. Note how we do not need to fetch the document text with `pt.text.get_text`, which further saves time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tynDMGCYJCUv"
      },
      "outputs": [],
      "source": [
        "br = pt.BatchRetrieve(index) % 50\n",
        "pipeline = (br >> indexed_epic.reranker())\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=['DPH', 'DPH >> EPIC (indexed)'],\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10, \"mrt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCF-DglUPrB-"
      },
      "source": [
        "## Tuning re-ranking threshold\n",
        "\n",
        "[Prior work suggests](https://arxiv.org/pdf/1904.12683.pdf) that the re-ranking cutoff threshold is an important model hyperparameter. Let's see how this parameter affects EPIC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osnHkyGtNy1m"
      },
      "outputs": [],
      "source": [
        "cutoffs = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "dph = pt.BatchRetrieve(index)\n",
        "res = pt.Experiment(\n",
        "    [dph % cutoff >> indexed_epic.reranker() for cutoff in cutoffs],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=[f'c={cutoff}' for cutoff in cutoffs],\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10, \"mrt\"]\n",
        ")\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvq1gw8VO_rp"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(res['name'], res['nDCG@10'], label='nDCG@10')\n",
        "plt.plot(res['name'], res['P(rel=2)@10'], label='P(rel=2)@10')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.plot(res['name'], res['mrt'])\n",
        "plt.ylabel('mrt')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8tu_yN4QRdh"
      },
      "source": [
        "It appears that the optimal re-ranking threshold for this collection is around 50-70. This also avoids excessive re-ranking time, which grows roughly linearly with larger thredhols. In pratice, this paramter should be tuned on a held-out validation set to avoid over-fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdUPsiYLX-L1"
      },
      "source": [
        "## monoT5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1UoVYscxzR_"
      },
      "source": [
        "The [monoT5](https://arxiv.org/abs/2003.06713) model scores documents using a causal language model. Let's see how this approach works on TREC COVID.\n",
        "\n",
        "The `MonoT5ReRanker` class from `pyterrier_t5` automatically loads a version of the monoT5 ranker that is trained on the MS MARCO passage dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuWuB44kQN2f"
      },
      "outputs": [],
      "source": [
        "from pyterrier_t5 import MonoT5ReRanker\n",
        "monoT5 = MonoT5ReRanker(text_field='title_abstract')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FI_KCicWDeB"
      },
      "outputs": [],
      "source": [
        "br = pt.BatchRetrieve(index) % 50\n",
        "pipeline = (br >> get_title_abstract >> monoT5)\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=['DPH', 'DPH >> T5'],\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10, \"mrt\"]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

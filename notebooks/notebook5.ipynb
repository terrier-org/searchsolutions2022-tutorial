{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEzvEdwBCu8j"
      },
      "source": [
        "# PyTerrier Search Solutions 2022 Tutorial Notebook - ColBERT\n",
        "\n",
        "This notebook provides experiences to attendees for building transformer pipelines in [PyTerrier](https://github.com/terrier-org/pyterrier).\n",
        "\n",
        "This notebook aims to demonstrate use of the [ColBERT dense retrieval](https://github.com/stanford-futuredata/ColBERT/tree/v0.2) for end-to-end indexing and retrieval in PyTerrier, as provided by the [*Pyterrier ColBERT*](https://github.com/terrierteam/pyterrier_colbert) plugin.\n",
        "\n",
        "In this notebook, you will experience indexing and retrieval using `pyterrier_colbert`.\n",
        "\n",
        "NB: ColBERT is memory hungry. For this reason, we are not able to demonstate ColBERT on corpora larger than Vaswani (11k abstracts) within the tight constraints of a Google Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Wk1OOx2R0-"
      },
      "source": [
        "# Setup\n",
        "\n",
        "In the following, we will set up the libraries required to execute the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMT4zyo2C0f7"
      },
      "source": [
        "## Python packages installation\n",
        "\n",
        "The following packages are installed to avoid warnings/errors during [PyTerrier](https://github.com/terrier-org/pyterrier) installation. [*Pyterrier ColBERT*](https://github.com/terrierteam/pyterrier_colbert) uses the following Python important packages:\n",
        "\n",
        "* `transfomers`,\n",
        "* `faiss-gpu` or `faiss-cpu` (NB: there is currently no `faiss-gpu` version for Python 3.11)\n",
        "\n",
        "> You can safely ignore the message about runtime restart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56dHbyAWCwyY"
      },
      "outputs": [],
      "source": [
        "!apt install --upgrade --quiet libomp-dev\n",
        "\n",
        "%pip install -q transformers\n",
        "%pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeCzaedsCu5t"
      },
      "source": [
        "## Pyterrier installation\n",
        "\n",
        "The following cell installs the latest release of the [PyTerrier](https://github.com/terrier-org/pyterrier) package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSUDPlRbDAHL"
      },
      "outputs": [],
      "source": [
        "%pip install -q python-terrier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxnKwBEoCu2_"
      },
      "source": [
        "## Pyterrier plugins installation\n",
        "\n",
        "We install the official version of the [*Pyterrier ColBERT*](https://github.com/terrierteam/pyterrier_colbert) plugin. You can safely ignore the package versioning errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-11-04T17:03:26.675Z"
        },
        "id": "OCE7EXbvIfFd"
      },
      "outputs": [],
      "source": [
        "%pip install -q git+https://github.com/terrierteam/pyterrier_colbert.git@no_mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8fum4I4Cu0a"
      },
      "source": [
        "# Preliminary steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-04T17:03:18.786713Z",
          "start_time": "2021-11-04T17:03:14.006237Z"
        },
        "id": "SRXwUvXvJFGk"
      },
      "outputs": [],
      "source": [
        "import pyterrier as pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GUjIYGfCuu8"
      },
      "source": [
        "## [Vaswani](http://ir.dcs.gla.ac.uk/resources/test_collections/npl/) dataset download\n",
        "\n",
        "The following cell downloads the [Vaswani](http://ir.dcs.gla.ac.uk/resources/test_collections/npl/) dataset that we will use in the remaining of the tutorial.\n",
        "\n",
        " We limit queries to just 50 topics to avoid RAM issues with ColBERT on Colab. ColBERT is **very** memory-hungry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-04T17:03:18.862564Z",
          "start_time": "2021-11-04T17:03:18.789838Z"
        },
        "id": "5s7EGfI2JRLW"
      },
      "outputs": [],
      "source": [
        "dataset = pt.get_dataset(\"vaswani\")\n",
        "topics = dataset.get_topics().head(50)\n",
        "qrels = dataset.get_qrels()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eS7k0tmCurm"
      },
      "source": [
        "# [ColBERT](https://github.com/stanford-futuredata/ColBERT) indexing\n",
        "\n",
        "We are going to index the [Vaswani](http://ir.dcs.gla.ac.uk/resources/test_collections/npl/) collection with [ColBERT](https://github.com/stanford-futuredata/ColBERT).\n",
        "\n",
        "The construction of this index takes some time. The following code:\n",
        "* downloads some additional BERT models;\n",
        "* processes the whole collection to compute the document embeddings, e.g, at most 180 embeddings per document;\n",
        "* performs the *training* of the IVFPQ [FAISS](https://github.com/facebookresearch/faiss) index supporting approximate nearest neightbour search.\n",
        "\n",
        "For 11,429 documents, the code computes 581,496 document embeddings, ~50.8 embeddings per document, in approximatively **5 minutes**.\n",
        "\n",
        "The following code can be use for indexing:\n",
        "\n",
        "```python\n",
        "!rm -rf /content/colbert_index\n",
        "\n",
        "import pyterrier_colbert.indexing\n",
        "\n",
        "colbert_indexer = pyterrier_colbert.indexing.ColBERTIndexer(checkpoint=checkpoint,\n",
        "                                                            index_root=\"/content\",\n",
        "                                                            index_name=\"colbert_index\",\n",
        "                                                            chunksize=3)\n",
        "colbert_indexer.index(dataset.get_corpus_iter())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3kRN1i3CunH"
      },
      "source": [
        "# Retrieval experiments\n",
        "\n",
        "Now we can load in the index and the learned model (which we will need for encoding queries). Index loading can take some time, as the [FAISS](https://github.com/facebookresearch/faiss) index needs to be loaded in main memory, as well as the document embeddings index.\n",
        "\n",
        "Lets prepare an experiment. Firstly, lets create in a BM25 baseline transformer, and the [ColBERT](https://github.com/stanford-futuredata/ColBERT) retrieve transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-04T17:03:19.067671Z",
          "start_time": "2021-11-04T17:03:18.864502Z"
        },
        "id": "-2UAASpBMl4x"
      },
      "outputs": [],
      "source": [
        "from pyterrier_colbert.ranking import ColBERTFactory\n",
        "\n",
        "bm25_terrier_stemmed = pt.BatchRetrieve.from_dataset('vaswani', 'terrier_stemmed', wmodel='BM25')\n",
        "\n",
        "factory = ColBERTFactory.from_dataset('vaswani', 'colbert_uog44k')\n",
        "factory.faiss_index_on_gpu = False  # we've only installed faiss-cpu, not faiss-gpu\n",
        "colbert_e2e = factory.end_to_end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC9sf67NOmGM"
      },
      "source": [
        "Lets give a look at the files downloaded. Firstly, note that ColBERT indexes into chunks - Vaswani is small enough to only need a single chunk, so we have only `0.pt` and no `1.pt` etc :\n",
        " - $x$ `.pt` - the document embeddings for each chunk\n",
        " - $x$ `.sample` - a sample of the document embeddings in that chunk - used for training FAISS, not needed at retrieval time\n",
        " - `doclens.` $x$ `.json` - the number of document embeddings per document.\n",
        " - `ivfpq.` $y$ `.faiss` - the FAISS index for all document embeddings\n",
        " - `docnos.pkl.gz` - the docno document metadata, used by PyTerrier_ColBERT to return docnos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVHdEDtsCuki"
      },
      "source": [
        "Now we are ready to run the experiments. We are going to retrieve the top 10 ranked documents for the official topics, and compute several effectiveness metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-04T17:02:40.374031Z",
          "start_time": "2021-11-04T17:02:31.055Z"
        },
        "id": "tjNydtH1MwcH"
      },
      "outputs": [],
      "source": [
        "pt.Experiment(\n",
        "    [bm25_terrier_stemmed % 10, colbert_e2e % 10],\n",
        "    topics,\n",
        "    qrels,\n",
        "    eval_metrics=[\"map\", \"recip_rank\", \"P_10\", \"ndcg_cut_10\", \"mrt\"],\n",
        "    names=['BM25', 'ColBERT'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SGHIbd6yhLR"
      },
      "source": [
        "So for this small dataset, ColBERT achieves a MAP is similar to BM25, a marginally higher P@10, but a lower MRR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlBEP8stcj1d"
      },
      "source": [
        "# Visualising ColBERT\n",
        "\n",
        "Let's dig a little deeper into which documents ColBERT retrieves and what it pays attention to in those documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M590wZjQcj1d"
      },
      "outputs": [],
      "source": [
        "query = 'what is the origin of covid 19'\n",
        "document = 'Origin of the COVID-19 virus has been intensely debated in the scientific community since the first infected cases were detected in December 2019. The disease has caused a global pandemic, leading to deaths of thousands of people across the world and thus finding origin of this novel coronavirus is'\n",
        "\n",
        "figure = factory.explain_text(query, document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTtbC4MRcj1d"
      },
      "source": [
        "This interaction figure shows how a query and a document interact. In particular:\n",
        "\n",
        "* the top sub-plot shows the contribution each query wordpiece to the document's score.\n",
        "* In the heatmap, darker colours indicate higher similarity between the query emebdding and the document embedding.\n",
        "* ColBERT uses max_sim operator - for each query embedding, only the most similar document embedding contributes to the final score of the document. For each query embedding, we put an \"X\" mark in the row of document embedding that is the source of that maximum similarity for that query embedding.\n",
        "* [MASK] tokens are extra tokens added to the query by ColBERT. We can observe which document embeddings these match with."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "PyTerrier (3.9)",
      "language": "python",
      "name": "pyt39"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}